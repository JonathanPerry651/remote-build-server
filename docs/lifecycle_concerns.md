# Client-Server Lifecycle Concerns

This document outlines the various lifecycle concerns involved in the interaction between the local Bazel Client (and Proxy) and the Remote Build Server infrastructure. Solutions are intentionally left blank for future definition.

## 1. Local Bootstrap & Discovery
**Concern**: How does the local Bazel Client (C++) discover and establish a connection to the local Proxy process?
*   **Solution**: The local Proxy runs in **Server Mode** (masquerading as the Bazel Server JVM) and listens on a **Unix Domain Socket** at `<output_base>/server/server.socket`. The Bazel Client (C++) automatically attempts to connect to this socket (or is directed to it via startup flags/standard conventions) instead of the traditional `command_port`.

**Concern**: How does the `jbazel` wrapper ensure the Proxy is running and healthy before invoking the Bazel Client?
*   **Solution**: The `jbazel` wrapper configures the Bazel Client to use the Proxy as the `server_javabase`. The Bazel Client itself manages the lifecycle of the server process: if the connection fails or the socket is missing, the Bazel Client launches the Proxy binary (as the "server") and waits for it to initialize the socket handling.

## 2. Session Provisioning
**Concern**: How does the Proxy identify the current user and workspace to request the correct remote resources?
*   **Solution**: The Proxy calculates a deterministic `RepoHash` based on the local workspace path (`--workspace_directory`). It combines this with the authenticated user ID (`RBS_USER_ID`) to form a unique Session Key. It then calls `GetServer(User, RepoHash)` on the Orchestrator.

**Concern**: How does the Orchestrator decide whether to create a new Agent or reuse an existing one?
*   **Solution**: The Orchestrator interacts with a persistent state store (Cloud Spanner/InMemory). It checks if an active Agent Pod exists for the `(User, RepoHash)` key. If yes, it returns the existing address (`xds://...`). If no, it delegates to the `KubernetesComputeService` to provision a new Namespace and Agent Pod, then registers its address.

## 3. Remote Process Management
**Concern**: How is the actual Bazel Server process launched inside the remote Agent container?
*   **Solution**: The Remote Agent listens for gRPC commands. Upon the first `RunBuild` or specific lifecycle request from the Proxy, the Agent spawns the actual Bazel Server process locally within the container, managing its stdio and lifecycle.

**Concern**: How are startup options (e.g., JVM flags, `--host_jvm_args`) propagated from the local client to the remote server?
*   **Solution**: The Proxy receives these arguments when it is launched by the Bazel Client. It serializes these startup options and includes them in the `GetServer` request (or the initial gRPC handshake with the Agent). The Agent then uses these exact flags when spawning the remote Bazel Server process.

## 4. Connection Tunneling
**Concern**: How is the gRPC connection established and maintained between the local Proxy and the remote Agent, potentially across network boundaries (e.g., NAT, Ingress)?
*   **Solution**: The connection uses **Proxyless gRPC** with **Google Cloud Traffic Director**. The Proxy dials a logical `xds://<namespace>.<project_id>` address. Traffic Director resolves this logical address to the specific endpoint (IP) of the Agent Pod in the build cluster, handling load balancing (if multiple replicas existed) and reachability.

**Concern**: How are connection drops or network interruptions handled during a long-running build?
*   **Solution**: gRPC's built-in reconnection logic handles transient network failures by re-establishing the underlying TCP connection to the resolved endpoint. Since the Agent Pod is persistent and stateful, the session resumes seamlessly. The Proxy can retry idempotent requests.

## 5. File System & Consistency
**Concern**: How does the system ensure the remote file system (source code, external repositories) matches the client's view?
*   **Solution**: We utilize **Bubblewrap** (sandbox) in the Agent Pod to construct a virtual filesystem that exactly mirrors the client's absolute paths. The underlying storage is a **Google Cloud Filestore** volume, which is mounted by both the client (via NFS) and the Agent (via PVC). This shared storage ensures source code and external repos are identical.

**Concern**: How are build artifacts (`bazel-out`) made accessible to the local client if needed?
*   **Solution**: Since the client mounts the same Filestore volume as the Agent, the `bazel-out` directory generated by the remote build is directly visible on the local filesystem via the NFS mount, allowing immediate access to binaries and logs.

## 6. Shutdown & Cleanup
**Concern**: What happens when the user runs `bazel shutdown` locally?
*   **Solution**: The Proxy receives the shutdown request from the client. It forwards a shutdown signal to the Remote Agent gRPC service. The Agent terminates the underlying Bazel Server process but keeps the Pod running (ready for future builds) until idle timeout. The Proxy then terminates itself.

**Concern**: How are idle remote resources (Pods, PVCs) detected and reclaimed?
*   **Solution**: The Orchestrator (or a sidecar/cron job) tracks the "Last Active Time" for each session. If a session exceeds a configured TTL (Time-To-Live) without activity, the `KubernetesComputeService` deletes the associated Namespace, effectively removing the Pod and cleaning up resources.

**Concern**: How does the system handle an abrupt disconnection (e.g., user kills the terminal)?
*   **Solution**: The Agent detects the gRPC stream cancellation. If the controlling Proxy disconnects unexpectedly, the Agent can abort the currently running build to save compute resources. If the client does not reconnect within a short grace period, the Agent may optionally shut down the Bazel Server process to reset state.
